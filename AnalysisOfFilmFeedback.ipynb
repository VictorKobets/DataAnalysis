{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ тональности киноотзывов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера мы воспользуемся набором данных, который содержит киноотзывы, оставленые на сайте IMDb (Internet Movie Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После распаковки нбор данных представляет собой две отдеьные папки с текстовыми файлами, одна\n",
    "папка - для обучения, а вторая - для тестирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая папка, в свою очередь, содержит две подпапки, одна называется pos, а другая - neg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "plt.rc('font', family = 'Verdana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/viktorkobets/Documents/aclImdb\r\n",
      "├── README\r\n",
      "├── imdb.vocab\r\n",
      "├── imdbEr.txt\r\n",
      "├── test\r\n",
      "│   ├── labeledBow.feat\r\n",
      "│   ├── neg\r\n",
      "│   ├── pos\r\n",
      "│   ├── urls_neg.txt\r\n",
      "│   └── urls_pos.txt\r\n",
      "└── train\r\n",
      "    ├── labeledBow.feat\r\n",
      "    ├── neg\r\n",
      "    ├── pos\r\n",
      "    ├── unsupBow.feat\r\n",
      "    ├── urls_neg.txt\r\n",
      "    ├── urls_pos.txt\r\n",
      "    └── urls_unsup.txt\r\n",
      "\r\n",
      "6 directories, 11 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -L 2 /Users/viktorkobets/Documents/aclImdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Папка pos содержит все положительные отзывы, каждый отзыв записан в виде отдельного текстового файла, папка neg содержит все отрицательные отзывы и также каждый отзыв представлен в виде отдельного текстового файла. Папка unsup сщдержит данные без отзывов, которые мы не будем использовать, и поэтому просто удаляем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: /Users/viktorkobets/Documents/aclImdb/train/unsup: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -r /Users/viktorkobets/Documents/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке sklearn есть вспомогательная функция load_files. Она позволяет загрузить файлы, для хранения которых используется такая структура папок, в которой каждая вложенная папка соответствует определенной метке. Сначала мы применим функцию load_files к обучающим данным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип text_train: <type 'list'>\n",
      "Длина text_train: 25000\n",
      "text_train[1]:\n",
      "Words can't describe how bad this movie is. I can't explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clichés, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won't list them here, but just mention the coloring of the plane. They didn't even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you're choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files('/Users/viktorkobets/Documents/aclImdb/train/')\n",
    "# load_files возвращает коллекцию, содержащую обучающие тексты и обучающие метки\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(u'Тип text_train: {}'.format(type(text_train)))\n",
    "print(u'Длина text_train: {}'.format(len(text_train)))\n",
    "print('text_train[1]:\\n{}'.format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b'<br />', b' ') for doc in text_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что объект text_train представляет собой список длиной 25000 элементов, в котором каждый элемент представляет собой строку, содержащую отзыв. Выше был напечатан только отзыв с индексом 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор данных был собран таким образом, чтобы положительный и отрицательный классы были сбалансированы, поэтому количество строк с положительными отзывами и количество строк с отрицательными отзывами в этом наборе одинаковое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество примеров на класс (обучение): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(u'Количество примеров на класс (обучение): {}'.format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество документов в тектовых данных: 25000\n",
      "Количество примеров на класс (тест): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "# аналогичным образом загружаем тестовые данные\n",
    "reviews_test = load_files('/Users/viktorkobets/Documents/aclImdb/test/')\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(u'Количество документов в тектовых данных: {}'.format(len(text_test)))\n",
    "print(u'Количество примеров на класс (тест): {}'.format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = [doc.replace(b'<br />', b' ') for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача, которую мы хотим решить, можно сформулировать следующим образом: каждому отзыву нужно присвоить метку \"положительный\" или \"отрицательный\" на основе анализа его текста. Это стандартная задача бинарной классификации. Однако текстовые данные представлены в формате, который модель машинного обучения не умеет обрабатывать. Нам нужно преобразовать строковое представление текста в числовое представление, к которому можно будет применить алгоритмы машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление текстовых данных в виде \"мешка слов\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя это представление, мы удаляем структуру исходного текста, например разбивку на главы и параграфы, знаки препинания,форматирование и лишь подсчитываем частоту встречаемости каждого слова в каждом документе корпуса. Удаление структуры и подсчет частоты каждого слова позволяет получить образное представление текста в виде \"мешка слов\". Получение из документа представления \"мешок слов\" включает три следующих этапа.\n",
    "\n",
    "1. Токенизация. Разбиваем каждый документ на слова, которые в нем встречаются (токены), например с помощью пробелов и знаков пунктуации.\n",
    "\n",
    "2. Построение словаря. Собираем словарь всех слов, которые появляются в любом из документов, и пронумеровываем их (например в алфавитном порядке).\n",
    "\n",
    "3. Создание разреженной матрицы. Для каждого документа подсчитываем, как часто каждое из слов, занесенное в словарь, встречается в этом документе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение модели \"мешок слов\" к синтетическому набору данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке sklearn модель \"мешок слов\" реализована в классе CountVectorizer, который и выполняет соответствующее преобразование. Для начала давайте применим класс CountVectorizer к синтетическому набору данных, состоящему из двух примеров, чтобы проиллюстрировать его работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words = ['The fool doth think he is wise,',\n",
    "               'but the wise man knows himself to be a fool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 13\n",
      "Содержимое словаря:\n",
      " {u'fool': 3, u'be': 0, u'he': 4, u'himself': 5, u'wise': 12, u'knows': 7, u'is': 6, u'but': 1, u'to': 11, u'the': 9, u'doth': 2, u'think': 10, u'man': 8}\n"
     ]
    }
   ],
   "source": [
    "print('Размер словаря: {}'.format(len(vect.vocabulary_)))\n",
    "print('Содержимое словаря:\\n {}'.format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь состоит из 13 слов, начинается со слова \"be\" и заканчивается словом \"wise\". Теперь, чтобы получить представление \"мешок слов\" для обучающих данных, достаточно вызвать метод trnsform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print('bag_of_words: {}'.format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы взглянуть на фактическое содержимое разряженной матрицы, мы можем преобразовать ее в \"плотный\" массив NumPy (который, помимо ненулевых элементов, хранит все нулевые элементы) с помощью метода toarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Плотное представление bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(u'Плотное представление bag_of_words:\\n{}'.format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель \"мешка слов\" для киноотзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(u'X_train:\\n{}'.format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь включает 74849 элементов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует еще один способ получить доступ к словарю - воспользоваться методом get_feature_name. Он возвращает удобный список, в котором каждый элемент соответствует одному признаку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество признаков: 74849\n",
      "Первые 20 признаков:\n",
      "[u'00', u'000', u'0000000000001', u'00001', u'00015', u'000s', u'001', u'003830', u'006', u'007', u'0079', u'0080', u'0083', u'0093638', u'00am', u'00pm', u'00s', u'01', u'01pm', u'02']\n",
      "Признаки с 20010 по 20030:\n",
      "[u'dratted', u'draub', u'draught', u'draughts', u'draughtswoman', u'draw', u'drawback', u'drawbacks', u'drawer', u'drawers', u'drawing', u'drawings', u'drawl', u'drawled', u'drawling', u'drawn', u'draws', u'draza', u'dre', u'drea']\n",
      "Каждый 2000-й признак:\n",
      "[u'00', u'aesir', u'aquarian', u'barking', u'blustering', u'b\\xeate', u'chicanery', u'condensing', u'cunning', u'detox', u'draper', u'enshrined', u'favorit', u'freezer', u'goldman', u'hasan', u'huitieme', u'intelligible', u'kantrowitz', u'lawful', u'maars', u'megalunged', u'mostey', u'norrland', u'padilla', u'pincher', u'promisingly', u'receptionist', u'rivals', u'schnaas', u'shunning', u'sparse', u'subset', u'temptations', u'treatises', u'unproven', u'walkman', u'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(u'Количество признаков: {}'.format(len(feature_names)))\n",
    "print('Первые 20 признаков:\\n{}'.format(feature_names[:20]))\n",
    "print('Признаки с 20010 по 20030:\\n{}'.format(feature_names[20010:20030]))\n",
    "print('Каждый 2000-й признак:\\n{}'.format(feature_names[::2000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
